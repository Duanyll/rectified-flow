{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation convertor\n",
    "\n",
    "**So Many Interpolationsâ€”What to Choose?**\n",
    "\n",
    "In the `samplers_2d_toys`, we mentioned that the interpolation coefficients $\\alpha_t, \\beta_t$ can be any time-differentiable functions satisfying $\\alpha_0 = \\beta_1 = 0$ and $\\alpha_1 = \\beta_0 = 1$. Typically, we also require $\\alpha_t$ to be monotonically increasing and $\\beta_t$ to be monotonically decreasing. We call this scheme *affine interpolations*. The interpolation process for *affine interpolations* is then defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\tX_t &= \\alpha_t \\cdot X_0 + \\beta_t \\cdot X_1 \\\\\n",
    "\t\\dot X_t &= \\dot \\alpha_t \\cdot X_0 + \\dot \\beta_t \\cdot X_1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The rectified flow velocity is given by:\n",
    "\n",
    "$$\n",
    "v(z, t) = \\mathbb{E}[ \\dot{X}_t \\mid X_t = z] = \\arg \\min_v \\int_0^1 \\mathbb{E} \\left[\\left\\| \\dot{\\alpha}_t X_1 + \\dot{\\beta}_t X_0 - v(X_t, t) \\right\\|^2 \\right] \\, \\mathrm{d}t\n",
    "$$\n",
    "\n",
    "At first glance, the velocity appears to be intrinsically connected with the interpolation coefficients, suggesting that the interpolation scheme must be chosen during training. However, in this section, we will demonstrate that it is actually possible to convert between different affine interpolation schemes at inference time, without the need to retrain the model.\n",
    "\n",
    "Let's first train a rectified flow with straight interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import copy\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch.distributions as dist\n",
    "\n",
    "from rectified_flow.utils import set_seed\n",
    "from rectified_flow.datasets.toy_gmm import TwoPointGMM\n",
    "\n",
    "from rectified_flow.rectified_flow import RectifiedFlow\n",
    "from rectified_flow.models.toy_mlp import MLPVelocityConditioned, MLPVelocity\n",
    "\n",
    "set_seed(0)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectified_flow.datasets.toy_gmm import TwoPointGMM\n",
    "\n",
    "n_samples = 50000\n",
    "pi_0 = dist.MultivariateNormal(torch.zeros(2, device=device), torch.eye(2, device=device))\n",
    "pi_1 = TwoPointGMM(x=15.0, y=2, std=0.3)\n",
    "D0 = pi_0.sample([n_samples])\n",
    "D1, labels = pi_1.sample_with_labels([n_samples])\n",
    "labels.tolist()\n",
    "\n",
    "from rectified_flow.flow_components.interpolation_solver import AffineInterp\n",
    "from rectified_flow.utils import visualize_2d_trajectories_plotly\n",
    "\n",
    "straight_interp = AffineInterp(\"straight\")\n",
    "spherical_interp = AffineInterp(\"spherical\")\n",
    "\n",
    "t = torch.linspace(0, 1, 1000)\n",
    "\n",
    "idx = torch.randperm(n_samples)[:1000]\n",
    "x_0 = D0[idx]\n",
    "x_1 = D1[idx]\n",
    "\n",
    "print(x_0.shape)\n",
    "\n",
    "straight_interp_list = []\n",
    "spherical_interp_list = []\n",
    "\n",
    "for t in np.linspace(0, 1, 50):\n",
    "\tx_t_straight, dot_x_t_straight = straight_interp.forward(x_0, x_1, t)\n",
    "\tx_t_spherical, dot_x_t_spherical = spherical_interp.forward(x_0, x_1, t)\n",
    "\tstraight_interp_list.append(x_t_straight)\n",
    "\tspherical_interp_list.append(x_t_spherical)\n",
    "\n",
    "visualize_2d_trajectories_plotly(\n",
    "\ttrajectories_dict={\"straight interp\": straight_interp_list, \"spherical interp\": spherical_interp_list},\n",
    "\tD1_gt_samples=D1[:5000],\n",
    "\tnum_trajectories=100,\n",
    "\ttitle=\"Interpolated Trajectories Visualization\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_trainer(rectified_flow, label = \"loss\", batch_size = 1024):\n",
    "    model = rectified_flow.velocity_field\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "    losses = []\n",
    "    for step in range(5000):\n",
    "        optimizer.zero_grad()\n",
    "        idx = torch.randperm(n_samples)[:batch_size]\n",
    "        x_0 = D0[idx].to(device)\n",
    "        x_1 = D1[idx].to(device)\n",
    "\n",
    "        loss = rectified_flow.get_loss(x_0, x_1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(f\"Epoch {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    plt.plot(losses, label=label)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectified_flow.models.toy_mlp import MLPVelocity\n",
    "\n",
    "straight_rf = RectifiedFlow(\n",
    "    data_shape=(2,),\n",
    "    velocity_field=MLPVelocity(2, hidden_sizes = [128, 128, 128]).to(device),\n",
    "    interp=straight_interp,\n",
    "    source_distribution=pi_0,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "rf_trainer(straight_rf, \"straight interp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectified_flow.samplers import EulerSampler\n",
    "\n",
    "euler_sampler_straight = EulerSampler(straight_rf, num_steps=100)\n",
    "\n",
    "euler_sampler_straight.sample_loop(seed=0, num_samples=1000)\n",
    "\n",
    "visualize_2d_trajectories_plotly(\n",
    "\ttrajectories_dict={\"1rf straight\": euler_sampler_straight.trajectories},\n",
    "\tD1_gt_samples=D1[:5000],\n",
    "\tnum_trajectories=100,\n",
    "\ttitle=\"Euler Sampler Visualization\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two affine interpolation processes defined with same coupling $(X_0, X_1)$:\n",
    "$$\n",
    "X_t = \\alpha_t X_1 + \\beta_t X_0 \\quad \\text{and} \\quad X_{t'}' = \\alpha_{t'}' X_1 + \\beta_{t'}' X_0.\n",
    "$$\n",
    "Our goal is to convert the rectified flow induced by $\\{X_t\\}$ into the one induced by $\\{X_{t'}'\\}$. The conversion procedure involves first matching the ratio $\\alpha/\\beta$ and then adjusting the scale accordingly.\n",
    "\n",
    "1. Matching Time $t$:\n",
    "\n",
    "   Note that\n",
    "   $$\n",
    "   \\dot{\\alpha}_t > 0, \\quad \\dot{\\beta}_t < 0, \\quad \\alpha_t, \\beta_t \\in [0, 1], \\quad \\forall t \\in [0, 1]\n",
    "   $$\n",
    "   This implies that $\\alpha_t / \\beta_t$ is strictly increasing on $[0,1]$. Hence, for any given $t$, there exists a unique $t'$ such that:\n",
    "   $$\n",
    "   \\frac{\\alpha_t}{\\beta_t} = \\frac{\\alpha'_{t'}}{\\beta'_{t'}}.\n",
    "   $$\n",
    "   Similarly, for any given $t'$, there exists a unique $t$. That said, $t$ and $t'$ can be uniquely matched in both directions. \n",
    "\n",
    "2. Match Scales:\n",
    "\n",
    "   After matching the ratios, consider:\n",
    "   $$\n",
    "   \\frac{X_t}{X_{t'}'} = \\frac{\\alpha_tX_1+\\beta_tX_0}{\\alpha'_{t'}X_1+\\beta'_{t'}X_0}= \\frac{\\alpha_t}{\\alpha'_{t'}} \\cdot \\frac{X_1+\\beta_t / \\alpha_t X_0}{X_1 + \\beta'_{t'}/\\alpha'_{t'}X_0}=\\frac{\\alpha_t}{\\alpha'_{t'}}.\n",
    "   $$\n",
    "   Define the scaling factor:\n",
    "   $$\n",
    "   \\omega_t := \\frac{\\alpha_t}{\\alpha'_{t'}} = \\frac{\\beta_t}{\\beta'_{t'}}\n",
    "   $$\n",
    "\n",
    "Given a pretrained 1-recitified flow induced from $\\{X_t\\}$, we want to transform it into the recitied flow induced from $\\{X_t'\\}$, and then solve the corresponding ODE for the transformed trajectories $\\{Z_{t'}'\\}$. Because the same transformation applies between $\\{Z_t\\}$ and $\\{Z_{t'}'\\}$ as between $\\{X_t\\}$ and $\\{X_{t'}'\\}$, we can proceed as follows:\n",
    "\n",
    "1. Use a binary search to find $t$ that matches the $\\alpha/\\beta$ ratio for each given $t$'. (As we're solving the ODE in the transformed space $\\{Z_{t'}'\\}$)\n",
    "2. Compute the scaling factor $\\omega_t$.\n",
    "3. Transform the velocity from the original rectified flow $\\{Z_t\\}$ to the new coordinates $\\{Z_{t'}'\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineInterpConverter:\n",
    "    def __init__(self, rectified_flow, target_interp):\n",
    "        self.rectified_flow = rectified_flow\n",
    "        self.source_interp = rectified_flow.interp\n",
    "        self.target_interp = target_interp\n",
    "\n",
    "    def match_time_and_scale(self, target_t):\n",
    "        \"\"\"Given t' from target_interp, return corresponding t from source_interp and scaling factor.\"\"\"\n",
    "        source_ratio = lambda s: self.source_interp.alpha(s) / self.source_interp.beta(s)\n",
    "        target_ratio = self.target_interp.alpha(target_t) / (self.target_interp.beta(target_t))\n",
    "        matched_t = self.binary_search_for_time(function=source_ratio, target=target_ratio)\n",
    "        scaling_factor = torch.where(\n",
    "            target_t == 0,\n",
    "            torch.ones_like(target_t),\n",
    "            torch.where(\n",
    "                target_t == 1,\n",
    "                torch.ones_like(target_t),\n",
    "                self.source_interp.alpha(matched_t) / self.target_interp.alpha(target_t)\n",
    "            )\n",
    "        )\n",
    "        matched_t = torch.where(\n",
    "            target_t == 0,\n",
    "            torch.zeros_like(target_t),\n",
    "            torch.where(\n",
    "                target_t == 1,\n",
    "                torch.ones_like(target_t),\n",
    "                matched_t\n",
    "            )\n",
    "        )\n",
    "        return matched_t, scaling_factor\n",
    "    \n",
    "    def get_transfored_velocity(self, transformed_rf, x_target, t_target):\n",
    "        \"\"\"Get the transformed velocity at x_t and time t.\"\"\"\n",
    "        t_target = transformed_rf.match_dim_with_data(t_target, x_target.shape, expand_dim=False)\n",
    "        t_source, scaling_factor = self.match_time_and_scale(t_target)\n",
    "        scaling_factor = transformed_rf.match_dim_with_data(scaling_factor, x_target.shape, expand_dim=True)\n",
    "        \n",
    "        original_velocity = self.rectified_flow.get_velocity(scaling_factor * x_target, t_source)\n",
    "        result = self.source_interp.solve(t=t_source, x_t=scaling_factor * x_target, dot_x_t=original_velocity)\n",
    "        \n",
    "        return self.target_interp.solve(t=t_target, x_0=result.x_0, x_1=result.x_1).dot_x_t\n",
    "    \n",
    "    def transform_rectified_flow(self):\n",
    "        transformed_rf = copy.deepcopy(self.rectified_flow)\n",
    "        transformed_rf.velocity_field = lambda x, t: self.get_transfored_velocity(transformed_rf, x, t)\n",
    "        transformed_rf.interp = self.target_interp\n",
    "        transformed_rf.source_interp = self.target_interp\n",
    "        return transformed_rf\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_search_for_time(function, target, tol=1e-7):\n",
    "        \"\"\" Binary search for the time t such that function(t) = target, tensor version.\n",
    "        target: torch.Tensor, shape (batch_size,)\n",
    "        function: takes in a tensor of shape (batch_size,) and returns a tensor of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        lower_bound = torch.zeros_like(target)\n",
    "        upper_bound = torch.ones_like(target)\n",
    "\n",
    "        mid_point = (upper_bound + lower_bound) / 2\n",
    "        current_value = function(mid_point)\n",
    "        iteration_count = 1\n",
    "        \n",
    "        while torch.max(torch.abs(current_value - target)) > tol and torch.max(torch.abs(upper_bound - lower_bound)) > tol:\n",
    "            upper_bound = torch.where(current_value >= target, mid_point, upper_bound)\n",
    "            lower_bound = torch.where(current_value <= target, mid_point, lower_bound)\n",
    "            \n",
    "            mid_point = (upper_bound + lower_bound) / 2.\n",
    "            current_value = function(mid_point)\n",
    "            iteration_count += 1\n",
    "            \n",
    "            if iteration_count > 1_000:\n",
    "                warnings.warn(\"Binary search exceeded maximum iterations.\", UserWarning)\n",
    "                break\n",
    "        \n",
    "        return mid_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure shows that, for a given $ t' $ in $\\{X'_{t'}\\}$, the corresponding $ t $ in $\\{X_t\\}$ and the scaling factor $\\omega_t = X_t / X'_{t'}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_type = \"spherical\"\n",
    "\n",
    "convert = AffineInterpConverter(straight_rf, AffineInterp(interp_type))\n",
    "\n",
    "t = torch.linspace(0, 1, 5000)\n",
    "matched_t, scaling_factor = convert.match_time_and_scale(t)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t, y=matched_t, mode='lines', name='matched_t'))\n",
    "fig.add_trace(go.Scatter(x=t, y=scaling_factor, mode='lines', name='scaling_factor'))\n",
    "fig.update_layout(\n",
    "    title=f'Matched Time and Scaling Factor, {interp_type} Interpolation',\n",
    "    xaxis_title='t',\n",
    "    yaxis_title='Value',\n",
    "    height=500,\n",
    "    width=700,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully converted the pretrained straight rectified flow into a spherical one.\n",
    "\n",
    "Next, let's perform sampling with different ODE steps. By varying the `num_steps`, we observe that as the `num_steps` increases, the MSE between the generated $Z_1$ and $Z_1'$ decreases.\n",
    "\n",
    "In fact, we can actually prove that if $Z_0=Z_0'=X_0$, then $Z_1=Z_1'$, where $Z_1 = \\int_0^1 v(Z_t,t) \\mathrm d t$ and $Z_1' = \\int_0^1 v'(Z_t',t) \\mathrm d t$, which means:\n",
    "$$\n",
    "(Z_0, Z_1) = (Z_0', Z_1')\n",
    "$$\n",
    "\n",
    "In other words, their rectified couplings are equivalent! Despite following very different trajectories, their outcomes turn out to be identical. \n",
    "\n",
    "However, even though different interpolations yield the same rectified coupling $(Z_0, Z_1)$, their flows follow different trajectories $\\{Z_t\\}$. When we simulate these trajectories, we cannot achieve perfect solutions because discretization is required. Generally, straighter trajectories of $\\{Z_t\\}$ are preferred, as they tend to reduce discretization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_interp = AffineInterp(\"spherical\")\n",
    "\n",
    "spherical_rf = AffineInterpConverter(straight_rf, target_interp).transform_rectified_flow()\n",
    "\n",
    "# Try different num_steps, e.g. [10, 50, 100, 1000]\n",
    "num_steps = 100\n",
    "euler_sampler1 = EulerSampler(straight_rf, num_steps=num_steps, num_samples=500)\n",
    "euler_sampler2 = EulerSampler(spherical_rf, num_steps=num_steps, num_samples=500)\n",
    "\n",
    "euler_sampler1.sample_loop(seed=0)\n",
    "euler_sampler2.sample_loop(seed=0)\n",
    "\n",
    "print(\"mse\", torch.mean((euler_sampler1.trajectories[-1] - euler_sampler2.trajectories[-1])**2))\n",
    "\n",
    "# zoom in to see they are really close\n",
    "visualize_2d_trajectories_plotly(\n",
    "\ttrajectories_dict={\"1rf straight\": euler_sampler1.trajectories, \"1rf spherical\": euler_sampler2.trajectories},\n",
    "\tD1_gt_samples=D1[:5000],\n",
    "\tnum_trajectories=100,\n",
    "\ttitle=\"Euler Sampler Visualization\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Two Rectified Flows: Nearly Same Results!\n",
    "\n",
    "Another interesting finding (also provable), is that given a sufficiently large dataset, different models or interpolations will converge to the same rectified coupling.\n",
    "\n",
    "We first demonstrate this phenomenon using a 2D toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "straight_rf = RectifiedFlow(\n",
    "    data_shape=(2,),\n",
    "    velocity_field=MLPVelocity(2, hidden_sizes = [128, 128, 128]).to(device),\n",
    "    interp=\"straight\",\n",
    "    source_distribution=pi_0,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "spherical_rf = RectifiedFlow(\n",
    "    data_shape=(2,),\n",
    "\tvelocity_field=MLPVelocity(2, hidden_sizes = [128, 128, 128]).to(device),\n",
    "\tinterp=\"spherical\",\n",
    "\tsource_distribution=pi_0,\n",
    "\tdevice=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "pi_0 = dist.MultivariateNormal(torch.zeros(2, device=device), torch.eye(2, device=device))\n",
    "pi_1 = TwoPointGMM(x=15.0, y=2, std=0.3)\n",
    "D0 = pi_0.sample([n_samples])\n",
    "D1, labels = pi_1.sample_with_labels([n_samples])\n",
    "\n",
    "set_seed(0)\n",
    "rf_trainer(straight_rf, \"straight interp\")\n",
    "set_seed(0)\n",
    "rf_trainer(spherical_rf, \"spherical interp\")\n",
    "\n",
    "sampler1 = EulerSampler(straight_rf, num_steps=1000, num_samples=500)\n",
    "sampler2 = EulerSampler(spherical_rf, num_steps=1000, num_samples=500)\n",
    "\n",
    "sampler1.sample_loop(seed=0)\n",
    "sampler2.sample_loop(seed=0)\n",
    "\n",
    "print(\"mse\", torch.mean((sampler1.trajectories[-1] - sampler2.trajectories[-1])**2))\n",
    "\n",
    "visualize_2d_trajectories_plotly(\n",
    "\ttrajectories_dict={\"straight interp\": sampler1.trajectories, \"spherical interp\": sampler2.trajectories},\n",
    "\tD1_gt_samples=D1[:5000],\n",
    "\tnum_trajectories=100,\n",
    "\ttitle=\"Euler Sampler Visualization\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_trainer_sample(rectified_flow, label = \"loss\", batch_size = 1024):\n",
    "    model = rectified_flow.velocity_field\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    losses = []\n",
    "    for step in range(6000):\n",
    "        optimizer.zero_grad()\n",
    "        x_0 = pi_0.sample([batch_size])\n",
    "        x_1, labels = pi_1.sample_with_labels([batch_size])\n",
    "        x_0 = x_0.to(device)\n",
    "        x_1 = x_1.to(device)\n",
    "\n",
    "        loss = rectified_flow.get_loss(x_0, x_1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(f\"Epoch {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    plt.plot(losses, label=label)\n",
    "    plt.legend()\n",
    "    \n",
    "set_seed(0)\n",
    "rf_trainer_sample(straight_rf, \"straight interp\")\n",
    "set_seed(0)\n",
    "rf_trainer_sample(spherical_rf, \"spherical interp\")\n",
    "\n",
    "sampler1 = EulerSampler(straight_rf, num_steps=1000, num_samples=500)\n",
    "sampler2 = EulerSampler(spherical_rf, num_steps=1000, num_samples=500)\n",
    "\n",
    "sampler1.sample_loop(seed=0)\n",
    "sampler2.sample_loop(seed=0)\n",
    "\n",
    "print(\"mse\", torch.mean((sampler1.trajectories[-1] - sampler2.trajectories[-1])**2))\n",
    "\n",
    "visualize_2d_trajectories_plotly(\n",
    "\ttrajectories_dict={\"straight interp\": sampler1.trajectories, \"spherical interp\": sampler2.trajectories},\n",
    "\tD1_gt_samples=D1[:5000],\n",
    "\tnum_trajectories=100,\n",
    "\ttitle=\"Euler Sampler Visualization\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
